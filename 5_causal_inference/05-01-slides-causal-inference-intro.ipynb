{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dowhy\n",
      "  Downloading dowhy-0.6-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 6.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from dowhy) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from dowhy) (1.6.3)\n",
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.8/site-packages (from dowhy) (0.12.2)\n",
      "Requirement already satisfied: sympy>=1.4 in /opt/conda/lib/python3.8/site-packages (from dowhy) (1.8)\n",
      "Collecting pydot>=1.4\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.8/site-packages (from dowhy) (2.5.1)\n",
      "Requirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.8/site-packages (from dowhy) (1.2.4)\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/conda/lib/python3.8/site-packages (from dowhy) (1.19.5)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /opt/conda/lib/python3.8/site-packages (from networkx>=2.0->dowhy) (4.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->dowhy) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.24->dowhy) (2021.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.8/site-packages (from pydot>=1.4->dowhy) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=0.24->dowhy) (1.15.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.8/site-packages (from sympy>=1.4->dowhy) (1.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->dowhy) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->dowhy) (1.0.1)\n",
      "Requirement already satisfied: patsy>=0.5 in /opt/conda/lib/python3.8/site-packages (from statsmodels->dowhy) (0.5.1)\n",
      "Installing collected packages: pydot, dowhy\n",
      "Successfully installed dowhy-0.6 pydot-1.4.2\n"
     ]
    }
   ],
   "source": [
    "!pip install dowhy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Science of Science Summer School (S4) 2021\n",
    "## Day 5: Causal Inference\n",
    "- Daniel E. Acuna, School of Information, Syracuse University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contents\n",
    "\n",
    "- Motivation\n",
    "- Potential outcomes framework\n",
    "- DAG\n",
    "- Backdoor criterion\n",
    "- Example identification\n",
    "- Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Movitations\n",
    "- We can use ML/AI/Deep learning to predict, but that does not mean that there is _causal_ effect (even if we have 100% accuracy!)\n",
    "\n",
    "Examples from Microsoft's DoWhy (https://github.com/microsoft/dowhy)\n",
    "- Will it work?\n",
    "  - Does a proposed change to a system improve people's outcomes?\n",
    "- Why did it work?\n",
    "  - What led to a change in a system's outcome?\n",
    "- What should we do?\n",
    "  - What changes to a system are likely to improve outcomes for people?\n",
    "- What are the overall effects?\n",
    "  - How does the system interact with human behavior?\n",
    "  - What is the effect of a system's recommendations on people's activity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recommendations\n",
    "\n",
    "- Most of this material taken from \"Causal Inference: The Mixtape\" https://mixtape.scunning.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An example\n",
    "\n",
    "![](images/intervention_data.png)\n",
    "from https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Potential outcomes framework\n",
    "\n",
    "> Rubin, Donald. 1974. “Estimating Causal Effects of Treatments in Randominzed and Nonrandomized Studies.” Journal of Educational Psychology 66 (5): 688–701."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Potential outcomes framework (2)\n",
    "\n",
    "- Potential outcomes are defined as $Y^1_i$ if unit $i$ received the treatment and $Y_i^0$ if the unit did not\n",
    "- Both outcomes have the same unit $i$ (it is the same person, scientist, journal, etc)\n",
    "- Therefore, *we can only observe* one state of the world.\n",
    "- Each unit has only one state of the world (either treatment occurred $Y^1$ or not $Y^0$)\n",
    "- Observable or \"actual\" outcomes $Y_i$ are different from potential outcomes. They are the outcomes that actually occur for unit $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Potential outcomes framework (3)\n",
    "\n",
    "- Let's define $D_i$ as the assignment of unit $i$, where $D_i = 1$ is unit assigned to treatment and $D_i = 0$ if not.\n",
    "- The following equation relates potential outcomes, observable outcomes, and assignment\n",
    "$$Y_i = D_i Y_i^1 + (1-D_i) Y_i^0$$\n",
    "- Notice that when $D_i=1$, we recover $Y_i = Y_i^1$ and when $D_i=0$ we recover $Y_i = Y_i^0$\n",
    "- Let's define the unit specific treatment effect as\n",
    "$$\\delta_i = Y_i^1 - Y_i^0$$\n",
    "- Try to understand what that means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Average treatment effects\n",
    "- For a population, we would like to know the _average treatment effect_\n",
    "\\begin{align}\n",
    "   ATE & = E[\\delta_i] \\nonumber      \\\\\n",
    "       & = E[Y^1_i - Y^0_i] \\nonumber \\\\\n",
    "       & = E[Y^1_i] - E[Y^0_i]        \n",
    "\\end{align}\n",
    "- Notice that we require to know both potential outcomes for each unit $i$.\n",
    "- **This is impossible**. This is known as the \"fundamental problem of causal inference\".\n",
    "- We must _infer_ $ATE$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Average treatment effect for the treatment group (ATT)\n",
    "- Alternatively, we might be interested in the treatment effect for the treated\n",
    "\n",
    "\\begin{align}\n",
    "   ATT & = E\\big[\\delta_i\\mid D_i=1\\big] \\nonumber                 \n",
    "   \\\\\n",
    "       & = E\\big[Y^1_i - Y^0_i \\mid D_i = 1\\big] \\nonumber          \n",
    "   \\\\\n",
    "       & = E\\big[Y^1_i\\mid D_i=1\\big] - E\\big[Y^0_i\\mid D_i=1\\big] \n",
    "\\end{align}\n",
    "\n",
    "- $E\\big[Y^0_i\\mid D_i=1\\big]$ is a **counterfactual**: _what would have happened to unit $i$ who received surgery had they received chemo instead?_\n",
    "- ATT will likely differ from ATE because the assignment might affect how they react to treatment (e.g., people who received the COVID-19 trial vaccines were in general more scientifically oriented, careful, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Average treatment effect for the untreated group (ATU)\n",
    "- Alternatively, we might be interested in the treatment effect for the treated\n",
    "\n",
    "\\begin{align}\n",
    "   ATU & = E\\big[\\delta_i\\mid D_i = 0\\big] \\nonumber                          \n",
    "   \\\\\n",
    "       & = E\\big[Y^1_i - Y^0_i\\mid D_i = 0\\big] \\nonumber                     \n",
    "   \\\\\n",
    "       & =E\\big[Y^1_i\\mid D_i=0\\big]-E\\big[Y^0_i\\mid D_i=0\\big] \n",
    "\\end{align}\n",
    "\n",
    "- Depending on the questions, ATE, ATT, or ATU, or all three, might be of interest. None of them, however, can be observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example data\n",
    "\n",
    "- Patients who have cancer, and two medical procedures or treatments. Surgery ($D_i=1$) or chemotherapy ($D_i=0$). Potential outcome is post-treatment life span.\n",
    "- Estimate \n",
    "$$ATT=E[Y_i^1]-E[Y_i^0] = 0.6$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](images/potential_outcomes_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example data: perfect doctor\n",
    "- Assume a perfect doctor who chooses treatment based on best prospects\n",
    "$ATT=E\\big[Y^1_i\\mid D_i=1\\big]-E\\big[Y^0_i\\mid D_i=1\\big]=4.4$\n",
    "$ATU=E\\big[Y^1_i\\mid D_i=0\\big]-E\\big[Y^0_i\\mid D_i=0\\big]=-3.2$\n",
    "However, a naive estimation based on observation\n",
    "$E[Y_1 \\mid D=1] - E[Y_0 \\mid D=0]=-0.4$ (it seems like surgey is worse!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "![](images/potential_outcomes_perfect_doctor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's going on\n",
    "\n",
    "- We can decompose our estimation:\n",
    "\n",
    "\\begin{align}\n",
    "\\underbrace{\\dfrac{1}{N_T} \\sum_{i=1}^n \\big(y_i\\mid d_i=1\\big)-\\dfrac{1}{N_C}\n",
    "   \\sum_{i=1}^n \\big(y_i\\mid d_i=0\\big)}_{ \\text{Simple Difference in Outcomes}}\n",
    "&= \\underbrace{E[Y^1] - E[Y^0]}_{ \\text{Average Treatment Effect}}\n",
    "\\\\\n",
    "&+ \\underbrace{E\\big[Y^0\\mid D=1\\big] - E\\big[Y^0\\mid D=0\\big]}_{ \\text{Selection bias}}\n",
    "\\\\\n",
    "& + \\underbrace{(1-\\pi)(ATT - ATU)}_{ \\text{Heterogeneous treatment effect bias}}\n",
    "\\end{align}\n",
    "where $\\pi$ is the share of patient who receive surgery\n",
    "- Selection bias: Difference between groups if both received chemo --- what if they never received a treatment in the first place?\n",
    "- Heterogeneous treatment effect bias difference between the surgery and chemo groups (counterfactually) weighted by how many people went to chemo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to eliminate these biases?\n",
    "\n",
    "\\begin{align}\n",
    "\\underbrace{\\dfrac{1}{N_T} \\sum_{i=1}^n \\big(y_i\\mid d_i=1\\big)-\\dfrac{1}{N_C}\n",
    "   \\sum_{i=1}^n \\big(y_i\\mid d_i=0\\big)}_{ \\text{Simple Difference in Outcomes}}\n",
    "&= \\underbrace{E[Y^1] - E[Y^0]}_{ \\text{Average Treatment Effect}}\n",
    "\\\\\n",
    "&+ \\underbrace{E\\big[Y^0\\mid D=1\\big] - E\\big[Y^0\\mid D=0\\big]}_{ \\text{Selection bias}}\n",
    "\\\\\n",
    "& + \\underbrace{(1-\\pi)(ATT - ATU)}_{ \\text{Heterogeneous treatment effect bias}}\n",
    "\\end{align}\n",
    "\n",
    "- Heterogeneous treatment effect bias: assume that treatment effects are the same for all units $\\forall i \\delta_i = \\delta $\n",
    "- Selection bias: **the goal of causal inference is to try to eliminate selection bias**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The \"simplest\" approach\n",
    "\n",
    "- Assume the conditional independence of potential outcomes with respect to assignment\n",
    "$$(Y_1 , Y_0) \\perp \\!\\!\\! \\perp D $$\n",
    "- This means that we assign patients to the surgery condition for reasons completely unrelated to potential gains in the surgery.\n",
    "- In our toy example, this is not true: surgery if $Y^1 > Y^0$, otherwise chemo.\n",
    "- Almost all observations of human behavior violate these assumptions: humans choose a treatment _because_ they expect some gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The \"simplest\" approach (2)\n",
    "- What if doctor chooses surgery independent of expected gains? Examples: alphabetical order, first half gets surgery, second half doesn't.\n",
    "- In general, if there is any source of external randomness for assignment, we can claim independence.\n",
    "- This would mean:\n",
    "\\begin{align}\n",
    "   E\\big[Y^1\\mid D=1\\big] - E\\big[Y^1\\mid D=0\\big]=0 \\\\\n",
    "   E\\big[Y^0\\mid D=1\\big] - E\\big[Y^0\\mid D=0\\big]=0 \n",
    "\\end{align}\n",
    "- No selection bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The \"simplest\" approach (3)\n",
    "\n",
    "- What about heterogeneity treatment bias.\n",
    "\\begin{gather}\n",
    "   ATT = E\\big[Y^1\\mid D=1\\big] - E\\big[Y^0\\mid D=1\\big]\n",
    "   \\\\\n",
    "   ATU = E\\big[Y^1\\mid D=0\\big] - E\\big[Y^0\\mid D=0\\big]\n",
    "\\end{gather}\n",
    "\n",
    "therefore\n",
    "\n",
    "\\begin{align}\n",
    "   ATT-ATU & =\\mathbf{E\\big[Y^1\\mid D=1\\big]}-E\\big[Y^0\\mid D=1\\big]    \\\\\n",
    "           & - \\mathbf{E\\big[Y^1 \\mid D=0\\big]}+ E\\big[Y^0\\mid D=0\\big] \\\\\n",
    "           & = 0                                                        \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Other assumptions\n",
    "- We usually need to assume that:\n",
    "   - All unites receive the same \"treatment\" dosage: easy to violate: some doctors are better than others\n",
    "   - No \"externalities\" from one unit to others (independence among units)\": easy to violate in networks. \n",
    "   - No \"general equilibrium\" effects: this means good generalization from experience to actual real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Directed Acyclical Graphs (DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why DAGs\n",
    "\n",
    "- Before starting causal inference, we must make sure that some conditions are met.\n",
    "- These conditions have to do with not allowing unwanted \"flows\" of information to go from unmeasured variables or measured variables that change the statistical structure of the problem.\n",
    "- To explain these ideas, we need to understand Directed Acyclical Graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG\n",
    "\n",
    "- The kinds of DAGs we need are _causal_ graph and not probabilistic graphical models*\n",
    "- They represent causal relationships between variables. For example\n",
    "![](images/dag1.png)\n",
    "- We have three random variables D, Y, and X. \n",
    "- D causes Y and X causes D and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG\n",
    "\n",
    "- We say that there is a path between the assignment $D$ and the outcome $Y$ like so $D \\rightarrow Y$\n",
    "- But we also say that there is a **backdoor path** from $D \\leftarrow X \\rightarrow Y$\n",
    "![](images/dag1.png)\n",
    "- We call $X$ a confounder because it jointly determines $D$ and $Y$\n",
    "- Leaving a \"backdoor\" open creates bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG\n",
    "\n",
    "- We say that there is a path between the assignment $D$ and the outcome $Y$ like so $D \\rightarrow Y$\n",
    "- But we also say that there is a **backdoor path** from $D \\leftarrow X \\rightarrow Y$\n",
    "![](images/dag1.png)\n",
    "- We call $X$ a confounder because it jointly determines $D$ and $Y$\n",
    "- Leaving a \"backdoor\" open creates bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG\n",
    "\n",
    "- Sometimes we have variables that we cannot observe\n",
    "- Here, we do not observe U, and therefore the backdoor is open.\n",
    "![](images/dag2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG\n",
    "- Sometimes, we have variables that are _colliders_. $X$ is a collider\n",
    "- We have a path $D \\rightarrow Y$ and backdoor path $D \\rightarrow X \\leftarrow Y$, **however, the backdoor path is _closed_ because of the collider** (causal effects do not flow from $X$.)\n",
    "![](images/dag3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG conditioning\n",
    "\n",
    "- Conditioning in DAGs is different from traditional probability distributions\n",
    "- To condition on this causal graph, we have to use different methods such as subclassification, matching, regression, etc.\n",
    "- We have to be careful, however:\n",
    "   - If a backdoor is _open_, we have to close it by **conditioning** on such variable\n",
    "   - If a backdoor is _closed_, we leave it alone. If we condition on it, it will become an **open backdoor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG example\n",
    "\n",
    "![](images/intervention_data_dag.png)\n",
    "from https://www.inference.vc/causal-inference-2-illustrating-interventions-in-a-toy-example/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DAG interventions\n",
    "- do-calculus by Judea Pearl is equivalent to the potential outcomes framework http://bayes.cs.ucla.edu/home.htm\n",
    "- it tends to be more popular in AI/ML settings but less applicable\n",
    "- potential outcomes is more popular with economics, public policy, statistics, but more applicable\n",
    "\n",
    "![](images/intervention_data_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Closing backdoors with subclassification\n",
    "\n",
    "- If we assume Conditional Independence Assumption (CIA)\n",
    "\n",
    "$$(Y^1,Y^0) \\perp \\!\\!\\! \\perp D\\mid X$$\n",
    "- and \n",
    "$$0<p(D=1 \\mid X) <1$$\n",
    "\n",
    "then we can assume independence between assignment and potential outcomes, and make the observations match the Average Treatment Effect (ATE)\n",
    "\n",
    "\\begin{align}\n",
    "   E\\big[Y^1\\mid D=1,X\\big]=E\\big[Y^1\\mid D=0,X\\big]\n",
    "   \\\\\n",
    "   E\\big[Y^0\\mid D=1,X\\big]=E\\big[Y^0\\mid D=0,X\\big]\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Closing backdoors with subclassification\n",
    "\n",
    "\n",
    "> Subclassification is a method of satisfying the backdoor criterion by weighting differences in means by strata-specific weights. These strata-specific weights will, in turn, adjust the differences in means so that their distribution by strata is the same as that of the counterfactual’s strata. This method implicitly achieves distributional balance between the treatment and control in terms of that known, observable confounder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Closing backdoors with subclassification\n",
    "\n",
    "- If we assume Conditional Independence Assumption (CIA)\n",
    "\n",
    "$$(Y^1,Y^0) \\perp \\!\\!\\! \\perp D\\mid X$$\n",
    "- and \n",
    "$$0<p(D=1 \\mid X) <1$$\n",
    "\n",
    "then we can assume independence between assignment and potential outcomes, and make the observations match the Average Treatment Effect (ATE)\n",
    "\n",
    "\\begin{align}\n",
    "   E\\big[Y^1\\mid D=1,X\\big]=E\\big[Y^1\\mid D=0,X\\big]\n",
    "   \\\\\n",
    "   E\\big[Y^0\\mid D=1,X\\big]=E\\big[Y^0\\mid D=0,X\\big]\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusions\n",
    "- There are many other ways of meeting the backdoor criterion:\n",
    "   - Exact matching, approximate matching (propensity score, CEM)\n",
    "   - Regression discontinuity\n",
    "   - Instrumental variables\n",
    "   - Difference-in-difference\n",
    "   - Synthetic controls\n",
    "- More in this book \"Causal Inference: The Mixtape\" https://mixtape.scunning.com/\n",
    "- Packages for computing causal effects: https://github.com/microsoft/dowhy\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "rise": {
   "chalkboard": {
    "color": [
     "rgb(250, 250, 250)",
     "rgb(250, 250, 250)"
    ]
   },
   "enable_chalkboard": true,
   "footer": "<h3><span style='color: orange'>Science of Science Summer School 2021</span> &#x2014; https://s4.scienceofscience.org &#x2014; (Virtually) hosted by School of Information Studies, Syracuse University: &#x2014; 07/26 - 08/6 <br>Vector art from <a href='https://www.freepik.com/'>https://www.freepik.com/</a></h3>",
   "header": "<h3>Day 5 - Causal Inference</h3>",
   "show_buttons_on_startup": false,
   "slideNumber": true,
   "theme": "white"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
