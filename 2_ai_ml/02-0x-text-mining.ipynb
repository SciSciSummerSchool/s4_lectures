{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1798408b",
   "metadata": {},
   "source": [
    "## Text Mining  \n",
    "\n",
    "For traditional data mining, data are often presented in a \"structured\" form: thoes data are presented in tabular form.   \n",
    "As we can see from the first line of data point we just imported, for a text mining task, we are dealing with a sequence of text, which is \"unstructured\". we will need to transform the text --- an \"unstructured\" form of data, into a \"structured\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91810",
   "metadata": {},
   "source": [
    "The first step to make text data \"structured\" is to tokenize text. To tokenize text is to segment text into smaller units: a word, a character or a punctuation. After recognizing all the tokens in a dataset, we can \"tell\" the computer what to look at when processing a line of text. One way to do it is to either count how many times a token appear in a line of text, or see whether a token appears in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4956",
   "metadata": {},
   "source": [
    "Load common packages for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd0ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac7027",
   "metadata": {},
   "source": [
    "Loading the citation dataset from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a933d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe558b00",
   "metadata": {},
   "source": [
    "Show the first 5 lines from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88dbc6",
   "metadata": {},
   "source": [
    "Get the first line of text. According to the label, it doesn't have citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bede893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['cur_sent'][0])\n",
    "print(df['cur_has_citation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d0bdc",
   "metadata": {},
   "source": [
    "Here, we import the functionality we need from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52a4a3",
   "metadata": {},
   "source": [
    "There are several setting we can choose for the text vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808acb",
   "metadata": {},
   "source": [
    "unigram term frequency vectorizer: each token is one word, the vectorizer count how many times a word appear in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1cc2a",
   "metadata": {},
   "source": [
    "unigram boolean vectorizer: instead of counting the word frequency, it checks whether the word appears in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581f6fc",
   "metadata": {},
   "source": [
    "unigram and bigram term frequency vectorizer: each token have up to 2 words. We are also using the built-in stop word list for English, so stopwords are not being counted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c878",
   "metadata": {},
   "source": [
    "tf-idf is a normalized version of word frequency count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38aee0",
   "metadata": {},
   "source": [
    "unigram tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75cf2e",
   "metadata": {},
   "source": [
    "fit vocabulary in texts and transform it into vectors. \"fit\" collects unique tokens into the vocabulary. \"transform\" converts each document to vector based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698879",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = unigram_count_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d06f4",
   "metadata": {},
   "source": [
    "The size of the vectorized dataset: there are 859636 data points and 261582 unigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae201db",
   "metadata": {},
   "source": [
    "As we can see here, a vecter for a line of text is sparse: most of the columns have 0 value because a vectorizer counts the appearance of all the tokens in the dataset even when a token is no in one particular line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b0802",
   "metadata": {},
   "source": [
    "The size of the vocabulary, in other words, the number of tokens in the dataset it is the size for each vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be975f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unigram_count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7889d5",
   "metadata": {},
   "source": [
    "## Classification Task with Vectorized Text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6277",
   "metadata": {},
   "source": [
    "Using the vectorized text, we can train a simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09bb47",
   "metadata": {},
   "source": [
    "In order to validate the model, we split the entire dataset into training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1355a3",
   "metadata": {},
   "source": [
    "Import logistic regression model and performance metrics from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335010da",
   "metadata": {},
   "source": [
    "Initialize the logistic regression model, setting the maximum iteration to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8c6a1",
   "metadata": {},
   "source": [
    "Fit the model with training split of the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a7580c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-a65f4e9fbac9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Initialize the logistic regression model, setting the maximum iteration to 10000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n\u001b[0;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[1;31m# TODO: remove in 1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_binned_statistic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkde\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgaussian_kde\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mqmc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_multivariate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47100a",
   "metadata": {},
   "source": [
    "Using the trained model, we make prediction with the text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1198",
   "metadata": {},
   "source": [
    "Calculate the f1 score for both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bc21",
   "metadata": {},
   "source": [
    "Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d81dee",
   "metadata": {},
   "source": [
    "Each word token correspond to a coefficient in the logistic regression. If a token is more important to the classification task, it is more likely to have a larger coefficient.In the following dataframe, we are sorting the tokens by the values of coefficients in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(unigram_count_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec635b8",
   "metadata": {},
   "source": [
    "## More Language Features with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c22d",
   "metadata": {},
   "source": [
    "There are also many more instereting feature we can get from a line of text aside from the frequency of words.  \n",
    "In the following section, we will explore more language features with the package spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Loading a pre-trained Pipeline \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the first line of sentence in our dataset with the loaded Pipeline\n",
    "tokens = nlp(df['cur_sent'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d927b",
   "metadata": {},
   "source": [
    "Print out the line of text we just passed to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3161507",
   "metadata": {},
   "source": [
    "Getting all the features generated by the Pipeline from the line of text we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_features = {}\n",
    "sentence_features['word'] = []\n",
    "sentence_features['lemma'] = []\n",
    "sentence_features['pos_tag'] = []\n",
    "sentence_features['shape'] = []\n",
    "sentence_features['is_alphabetic'] = []\n",
    "sentence_features['is_stopword'] = []\n",
    "\n",
    "for token in tokens:\n",
    "    sentence_features['word'].append(token.text)\n",
    "    sentence_features['lemma'].append(token.lemma_)\n",
    "    sentence_features['pos_tag'].append(token.pos_)\n",
    "    sentence_features['shape'].append(token.shape_)\n",
    "    sentence_features['is_alphabetic'].append(token.is_alpha)\n",
    "    sentence_features['is_stopword'].append(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a693d1",
   "metadata": {},
   "source": [
    "In the table below, we see that the Pipeline tokenized the text into words.  \n",
    "\"lemma\" is the base form of the token (word)  \n",
    "\"pos_tag\" is the pos-tagging tags for a token  \n",
    "\"shape\" shows the visual shape of the token (uppercase or lowercase, punctuation, digits)  \n",
    "\"is alphabetic\" shows whether a token is alphabetic  \n",
    "\"is stopword\" shows whether a token is a stopword  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(sentence_features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ccd9c",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91473c0",
   "metadata": {},
   "source": [
    "Try using tfidf vectors to train the logistic regression. In that case, what are the most important tokens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
