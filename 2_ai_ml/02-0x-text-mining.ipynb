{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2141a23",
   "metadata": {},
   "source": [
    "## Text Mining  \n",
    "\n",
    "For traditional data mining, data are often presented in a \"structured\" form: thoes data are presented in tabular form.   \n",
    "As we can see from the first line of data point we just imported, for a text mining task, we are dealing with a sequence of text, which is \"unstructured\". we will need to transform the text --- an \"unstructured\" form of data, into a \"structured\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780db028",
   "metadata": {},
   "source": [
    "The first step to make text data \"structured\" is to tokenize text. To tokenize text is to segment text into smaller units: a word, a character or a punctuation. After recognizing all the tokens in a dataset, we can \"tell\" the computer what to look at when processing a line of text. One way to do it is to either count how many times a token appear in a line of text, or see whether a token appears in the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common packages for data transformation\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the citation dataset from the data folder\n",
    "df = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c4adeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the first 5 lines from the top\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c3c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first line of text. According to the label, it doesn't have citation\n",
    "print(df['cur_sent'][0])\n",
    "print(df['cur_has_citation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91694fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we import the functionality we need from scikit-learn:\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several setting we can choose for the text vectorizer:\n",
    "\n",
    "#  unigram term frequency vectorizer: each token is one word, the vectorizer count how many times a word appear in the text\n",
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False)\n",
    "\n",
    "#  unigram boolean vectorizer: instead of counting the word frequency, it checks whether the word appears in the text\n",
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True)\n",
    "\n",
    "# unigram and bigram term frequency vectorizer: each token have up to 2 words. \n",
    "# We are also using the built-in stop word list for English, so stopwords are not being counted \n",
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf is a normalized version of word frequency count \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#  unigram tfidf vectorizer\n",
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit vocabulary in texts and transform it into vectors.\n",
    "# \"fit\" collects unique tokens into the vocabulary\n",
    "# \"transform\" converts each document to vector based on the vocabulary\n",
    "word_vector = unigram_count_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vectorized dataset: there are 859636 data points and 261582 unigram tokens\n",
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see here, a vecter for a line of text is sparse: \n",
    "# most of the columns have 0 value because a vectorizer counts the appearance of all the tokens in the dataset \n",
    "# even when a token is no in one particular line of text\n",
    "print(word_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be975f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size of the vocabulary, in other words, the number of tokens in the dataset\n",
    "# it is the size for each vector \n",
    "print(len(unigram_count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad11d2",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7889d5",
   "metadata": {},
   "source": [
    "## Classification Task with Vectorized Text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6277",
   "metadata": {},
   "source": [
    "Using the vectorized text, we can train a simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to validate the model, we split the entire dataset into training dataset and testing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import logistic regression model and performance metrics from scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model, setting the maximum iteration to 10000\n",
    "clf = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "# Fit the model with training split of the vectorized data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model, we make prediction with the text split\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the f1 score for both positive and negative class\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each word token correspond to a coefficient in the logistic regression. \n",
    "# If a token is more important to the classification task, it is more likely to have a larger coefficient.\n",
    "# In the following dataframe, we are sorting the tokens by the values of coefficients in descending order.\n",
    "pd.concat([pd.DataFrame(unigram_count_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28be66c",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc291a31",
   "metadata": {},
   "source": [
    "Let's try using a different tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9cc0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word_vector = unigram_tfidf_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d5c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logistic regression model, setting the maximum iteration to 10000\n",
    "clf = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "# Fit the model with training split of the vectorized data\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8dc205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model, we make prediction with the text split\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5151680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the f1 score for both positive and negative class\n",
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each word token correspond to a coefficient in the logistic regression. \n",
    "# If a token is more important to the classification task, it is more likely to have a larger coefficient.\n",
    "# In the following dataframe, we are sorting the tokens by the values of coefficients in descending order.\n",
    "pd.concat([pd.DataFrame(unigram_tfidf_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ff587",
   "metadata": {},
   "source": [
    "With different vectorization methods, we will get different performance for our model and different model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161126a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc6ce32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec635b8",
   "metadata": {},
   "source": [
    "## More Language Features with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c22d",
   "metadata": {},
   "source": [
    "There are also many more instereting feature we can get from a line of text aside from the frequency of words.  \n",
    "In the following section, we will explore more language features with the package spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Loading a pre-trained Pipeline \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the first line of sentence in our dataset with the loaded Pipeline\n",
    "tokens = nlp(df['cur_sent'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the line of text we just passed to the Pipeline\n",
    "print(tokens.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all the features generated by the Pipeline from the line of text we passed\n",
    "\n",
    "sentence_features = {}\n",
    "sentence_features['word'] = []\n",
    "sentence_features['lemma'] = []\n",
    "sentence_features['pos_tag'] = []\n",
    "sentence_features['shape'] = []\n",
    "sentence_features['is_alphabetic'] = []\n",
    "sentence_features['is_stopword'] = []\n",
    "\n",
    "for token in tokens:\n",
    "    sentence_features['word'].append(token.text)\n",
    "    sentence_features['lemma'].append(token.lemma_)\n",
    "    sentence_features['pos_tag'].append(token.pos_)\n",
    "    sentence_features['shape'].append(token.shape_)\n",
    "    sentence_features['is_alphabetic'].append(token.is_alpha)\n",
    "    sentence_features['is_stopword'].append(token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the table below, we see that the Pipeline tokenized the text into words.\n",
    "# \"lemma\" is the base form of the token (word)\n",
    "# \"pos_tag\" is the pos-tagging tags for a token\n",
    "# \"shape\" shows the visual shape of the token (uppercase or lowercase, punctuation, digits)\n",
    "# \"is alphabetic\" shows whether a token is alphabetic\n",
    "# \"is stopword\" shows whether a token is a stopword\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(sentence_features).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
