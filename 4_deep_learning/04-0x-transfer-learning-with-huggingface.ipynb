{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eddd1a5",
   "metadata": {},
   "source": [
    "# Transfer Learning with Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c2c62",
   "metadata": {},
   "source": [
    "```transformers``` is a python package by Huggingface (https://huggingface.co/transformers/)    \n",
    "\n",
    "With the pacakge, we can load pre-trained packages that can interacts with pytorch and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c2aa2",
   "metadata": {},
   "source": [
    "Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93352d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1aa2f5",
   "metadata": {},
   "source": [
    "Loading part of the ACL-ARC citation prediction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebe477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_json('../../training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9e6f95",
   "metadata": {},
   "source": [
    "Loading pre-trained model and tokenizer.  \n",
    "\n",
    "SPECTER pre-trained model in ```transformers``` package is based a document-level representation learning method using Citation-informed Transformers. The fine-tuned model generates document-level embedding of scientific documents of high quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2fb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('allenai/specter')\n",
    "model = AutoModel.from_pretrained('allenai/specter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d2630",
   "metadata": {},
   "source": [
    "Take one sentenece for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c26434b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = training['cur_sent'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f57244b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the system consists of two linguistically significant parts a machine lexicon residing on a direct access device and a program package'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955651a",
   "metadata": {},
   "source": [
    "The tokenizer is spliting the sentence into tokens. The tokenizer will first split the sentence into smaller units: for example, words, then look up a dictionary to find an index corresponds to a word, as shown in ```\"token (int)\"```. There will be cases when a word is not in the dictionary. The tokenizer will then further split the word into smaller sub-words recursively until smaller sub-words will be found in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a751b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token (str): ['[CLS]', 'the', 'system', 'consists', 'of', 'two', 'linguistic', '##ally', 'significant', 'parts', 'a', 'machine', 'lexicon', 'residing', 'on', 'a', 'direct', 'access', 'device', 'and', 'a', 'program', 'package', '[SEP]']\n",
      "token (int): [101, 111, 483, 3777, 125, 532, 12106, 397, 700, 3983, 105, 4623, 28374, 29295, 201, 105, 1464, 2131, 3476, 136, 105, 2457, 8553, 102]\n",
      "type       : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sentence)\n",
    "\n",
    "print(\"token (str): {}\".format(tokenizer.convert_ids_to_tokens(tokens['input_ids'])))\n",
    "print(\"token (int): {}\".format(tokens['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d81220",
   "metadata": {},
   "source": [
    "After tokenizing, we will pass the tokens to the model to generate sentence representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5da3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_pt = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**tokens_pt)\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c147e3",
   "metadata": {},
   "source": [
    "There are two types of input for BERT-based models. one is called \"last hidden state\", which is the sequence of hidden-states at the output of the last layer of the model. Each token correspond to a last hidden-state output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a5ea18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24, 768])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e81b864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7792,  0.8333,  0.2410,  ...,  0.4887, -0.0467,  0.3051],\n",
       "         [ 0.2098,  0.0838,  0.1247,  ...,  0.5135,  0.5461, -0.1118],\n",
       "         [-0.5035,  0.1904,  0.6871,  ...,  0.8021,  0.1066,  0.3445],\n",
       "         ...,\n",
       "         [-1.4837,  0.8487,  0.6305,  ...,  0.5358,  0.8152,  0.3259],\n",
       "         [-1.4575,  0.7227,  0.6408,  ..., -0.5666,  0.5255,  0.2348],\n",
       "         [-0.9408,  0.8182,  0.2210,  ...,  0.5561,  0.0099,  0.3110]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ed746",
   "metadata": {},
   "source": [
    "Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence prediction (classification) objective during pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accca43f",
   "metadata": {},
   "source": [
    "The other type is call \"pooler output\", which is the last layer hidden-state of the first token of the sequence further processed by a Linear layer and a Tanh activation function. The first token of the sequence is related to pre-training classification task, so pooler output is often used for classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afb306f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "195e94e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2540,  0.2876, -0.2340,  0.1843,  0.9807, -0.0738,  0.5956,  0.1019,\n",
       "         -0.4288, -0.2507,  0.7336, -0.0908, -0.3881,  0.3673,  0.2697, -0.4390,\n",
       "          0.1150,  0.2368,  0.2760, -0.1142, -0.1232, -0.0835, -0.0471, -0.0462,\n",
       "         -0.0721,  0.1732, -0.7120,  0.9663,  0.1583,  0.4891, -0.2640,  0.0396,\n",
       "          0.2055,  0.1348, -0.3211,  0.4341,  0.1282,  0.2765,  0.3996, -0.0257,\n",
       "          0.7475, -0.9900,  0.8704, -0.1534, -0.2722,  0.5214, -0.2670,  0.9717,\n",
       "         -0.2234,  0.5857, -0.0092,  0.2714, -0.5649,  0.0629, -0.0214,  0.9859,\n",
       "          0.9603,  0.9990, -0.0141,  0.9154,  0.9727,  0.1833, -0.3122,  0.4822,\n",
       "          0.2448, -0.5942, -0.1967,  0.3873, -0.3230, -0.5539, -0.0785, -0.5370,\n",
       "          0.0573,  0.9926,  0.1674, -0.0150, -0.5095,  0.9821, -0.5030, -0.0936,\n",
       "         -0.0617,  0.0863,  0.4232, -0.2958,  0.0321,  0.9949,  0.1428, -0.4704,\n",
       "         -0.6120,  0.2401, -0.1410, -0.0368, -0.5527,  0.2229, -0.1670,  0.4611,\n",
       "         -0.8674,  0.4148,  0.2540,  0.0040,  0.9734,  0.3062,  0.5804, -0.3140,\n",
       "          0.3614, -0.8987, -0.2054,  0.5301,  0.9665,  0.9709, -0.2132,  0.6443,\n",
       "         -0.7352,  0.0423, -0.3298, -0.1448,  0.9976,  0.0864,  0.2470, -0.1868,\n",
       "          0.4663,  0.9989,  0.1231, -0.3406, -0.2667,  0.0351, -0.0549,  0.9981,\n",
       "         -0.2029, -0.9404,  0.3237, -0.2241, -0.2237,  0.9726, -0.2331,  0.2208,\n",
       "          0.8211, -0.0654,  0.7536,  0.5681,  0.2234, -0.2537, -0.3332, -0.1928,\n",
       "         -0.0194, -0.7974, -0.1055,  0.2386,  0.0880, -0.1882, -0.1621,  0.7882,\n",
       "         -0.4989, -0.0757, -0.1775,  0.4679,  0.4092, -0.9202,  0.5358, -0.0342,\n",
       "          0.1749, -0.2221,  0.1034,  0.0132,  0.0870, -0.0424,  0.4273,  0.1079,\n",
       "          0.3950,  0.0358,  0.0977,  0.0291,  0.4182, -0.4533,  0.9474, -0.6015,\n",
       "          0.2756, -0.2275, -0.8440, -0.3227,  0.2134, -0.5833, -0.0594, -0.5453,\n",
       "         -0.9981,  0.6997,  0.5372, -0.2270,  0.2604, -0.0241,  0.2780, -0.9904,\n",
       "         -0.9986,  0.2847, -0.7973, -0.3765,  0.0616,  0.4219,  0.3810,  0.0909,\n",
       "         -0.1040,  0.1599,  0.8349, -0.0502,  0.2543, -0.3955, -0.0644,  0.1131,\n",
       "          0.0365, -0.1133, -0.4352, -0.9983, -0.4245,  0.9993,  0.9151, -0.0978,\n",
       "         -0.2971, -0.0204,  0.8726, -0.9462, -0.5681, -0.9467, -0.2660,  0.0600,\n",
       "         -0.3476, -0.8350, -0.6509,  0.4248, -0.9659, -0.9829,  0.1052,  0.1118,\n",
       "          0.3135, -0.9516, -0.4460, -0.0915,  0.9726,  0.2462,  0.5737, -0.2196,\n",
       "         -0.6396,  0.9293,  0.6131,  0.1769, -0.2986,  0.9940,  0.9768,  0.9876,\n",
       "         -0.1245,  0.0717,  0.9820, -0.2210, -0.2181, -0.2468, -0.2901,  0.0839,\n",
       "         -0.2099,  0.1811, -0.4076,  0.9309, -0.9946, -0.9812,  0.3038,  0.9221,\n",
       "          0.1336, -0.1668,  0.7896,  0.7370, -0.4608,  0.1592,  0.2037, -0.9536,\n",
       "         -0.5567, -0.1554,  0.2901, -0.6181,  0.0926, -0.0555, -0.1841,  0.0127,\n",
       "         -0.0818, -0.9294,  0.1481,  0.7043, -0.0663, -0.8370, -0.2717,  0.3031,\n",
       "         -0.9954,  0.1992,  0.1649,  0.9853,  0.9743, -0.4543,  0.3849, -0.5890,\n",
       "         -0.0553,  0.4695, -0.1616,  0.1419,  0.1471, -0.6742,  0.0450,  0.1066,\n",
       "          0.9905,  0.5066, -0.6183, -0.0902,  0.9158,  0.0683,  0.2939,  0.9860,\n",
       "         -0.4804, -0.9221,  0.0663,  0.1202, -0.6176, -0.1173, -0.0480, -0.2267,\n",
       "          0.1824, -0.9420, -0.3907,  0.8110, -0.2924, -0.3155,  0.3419, -0.0302,\n",
       "          0.2917,  0.0043,  0.1224,  0.7858,  0.7752, -0.4556,  0.0439, -0.4504,\n",
       "          0.4731,  0.0297, -0.3738,  0.5597,  0.2590, -0.9978,  0.8248,  0.3577,\n",
       "          0.2207, -0.8699, -0.6528, -0.2607,  0.1880,  0.1435,  0.6341,  0.2636,\n",
       "          0.3575, -0.3323,  0.1592, -0.1589, -0.0313, -0.2202,  0.4285,  0.6086,\n",
       "          0.3393,  0.6044,  0.6264, -0.7374, -0.0687,  0.4876, -0.9433, -0.2336,\n",
       "          0.0601, -0.3812, -0.2502,  0.4234,  0.0060,  0.2829,  0.6014, -0.0691,\n",
       "          0.2484,  0.3167,  0.0893,  0.3332, -0.4623, -0.2439, -0.0564,  0.0237,\n",
       "          0.4406, -0.0159,  0.9442, -0.1180,  0.1481,  0.0629,  0.1670,  0.2655,\n",
       "          0.5528, -0.9991,  0.8784,  0.2520, -0.1310,  0.1479,  0.2803, -0.2899,\n",
       "          0.9418, -0.6539,  0.8991, -0.4881,  0.5900, -0.0897,  0.8655, -0.0311,\n",
       "         -0.9808,  0.4560, -0.4964, -0.3416,  0.9153, -0.4716, -0.2056, -0.9926,\n",
       "         -0.4337, -0.1470,  0.2180,  0.0414, -0.9998,  0.1893, -0.3396,  0.9977,\n",
       "         -0.1474, -0.7320,  0.7534, -0.7237,  0.1792,  0.0585,  0.1566,  0.2069,\n",
       "          0.2599,  0.9473,  0.3510,  0.4163, -0.9040,  0.3671,  0.5156, -0.9074,\n",
       "         -0.7967,  0.2635,  0.1155, -0.1518, -0.1064,  0.1365,  0.9887, -0.2609,\n",
       "         -0.2132, -0.3506, -0.4817,  0.0271, -0.1462,  0.4657,  0.9962,  0.4421,\n",
       "         -0.1614, -0.3134, -0.5315, -0.3874, -0.0178, -0.1453,  0.0872,  0.4897,\n",
       "          0.0218, -0.0818, -0.9978,  0.4238, -0.0248, -0.0228,  0.9834, -0.0713,\n",
       "          0.0875,  0.1333,  0.4661,  0.8784, -0.0261,  0.3097, -0.9946,  0.4278,\n",
       "          0.1083, -0.1609, -0.0980, -0.0754,  0.1872, -0.4970,  0.5768,  0.9029,\n",
       "         -0.4664,  0.6022,  0.7079, -0.0296,  0.6094,  0.1446,  0.1347, -0.1100,\n",
       "         -0.0081,  0.0914, -0.1857, -0.9063,  0.8355,  0.9597, -0.9834, -0.1876,\n",
       "          0.5780, -0.7184, -0.0134, -0.4891, -0.1614,  0.0143,  0.2608,  0.0620,\n",
       "         -0.5828, -0.1155,  0.9946, -0.2297,  0.1991,  0.0986,  0.9935, -0.1308,\n",
       "         -0.1398,  0.0337,  0.9646,  0.0960, -0.9986, -0.2467, -0.7912,  0.8797,\n",
       "         -0.1822,  0.2826,  0.3063,  0.0248,  0.7242, -0.3092, -0.1956, -0.3804,\n",
       "          0.2518,  0.4121, -0.1815,  0.8954,  0.4198, -0.4584, -0.0149, -0.8438,\n",
       "         -0.2212, -0.1525, -0.0788,  0.9306,  0.5413,  0.3467, -0.9958,  0.2687,\n",
       "         -0.6621,  0.3346,  0.4567,  0.1099,  0.4235, -0.3999, -0.3807,  0.0016,\n",
       "          0.3964, -0.3530, -0.1847,  0.3040,  0.0703,  0.9885, -0.2292,  0.9760,\n",
       "         -0.5129,  0.6609, -0.9907,  0.0210, -0.9788, -0.9913,  0.9879, -0.0735,\n",
       "         -0.3213,  0.0421,  0.1348, -0.1937,  0.1513,  0.2944, -0.2698,  0.1128,\n",
       "          0.9690, -0.2280,  0.0764,  0.8821,  0.2699,  0.3515,  0.2792,  0.4884,\n",
       "         -0.2056,  0.8810,  0.0241,  0.1822, -0.3405, -0.5510, -0.5507,  0.2128,\n",
       "         -0.9675, -0.3781, -0.1863, -0.2055,  0.0706, -0.2402,  0.3177,  0.2776,\n",
       "         -0.9858,  0.4055, -0.6804,  0.3074,  0.9836,  0.9162, -0.9423,  0.9968,\n",
       "          0.1169, -0.1069, -0.1769, -0.4430, -0.1541, -0.3934, -0.6822,  0.0375,\n",
       "         -0.4234,  0.3875, -0.3690,  0.3496,  0.2047,  0.0966, -0.9481, -0.1774,\n",
       "          0.2300, -0.3451, -0.3194,  0.6338,  0.2534, -0.3436, -0.6912, -0.2654,\n",
       "         -0.3509, -0.2875, -0.0380, -0.0385,  0.2639, -0.3532, -0.0753, -0.0321,\n",
       "          0.1547, -0.4518, -0.7131, -0.0478,  0.8357,  0.1291,  0.1489,  0.2398,\n",
       "          0.4676,  0.3961, -0.4151,  0.3692,  0.9924, -0.1740,  0.1712, -0.4195,\n",
       "          0.2856,  0.1235, -0.0703,  0.6191,  0.7352,  0.3093,  0.5028, -0.7036,\n",
       "          0.0397,  0.0890,  0.0433, -0.0899,  0.5123, -0.3035,  0.1784, -0.3710,\n",
       "         -0.1225,  0.7314, -0.1726, -0.0383,  0.4610, -0.0571, -0.2686,  0.1182,\n",
       "          0.9018, -0.7665, -0.2261,  0.2687, -0.1704, -0.1216,  0.4599,  0.2621,\n",
       "         -0.0772, -0.0677,  0.0180,  0.0824,  0.3164,  0.3490, -0.3687, -0.2161,\n",
       "          0.9189, -0.3945,  0.0470,  0.9447,  0.6513,  0.1455,  0.0229, -0.2016,\n",
       "         -0.3018, -0.4551,  0.5416, -0.3539, -0.4758,  0.4278,  0.9955, -0.2392,\n",
       "         -0.0386,  0.9153, -0.9955,  0.2251,  0.0552, -0.0265,  0.4646, -0.5564,\n",
       "          0.5407, -0.0337, -0.5776, -0.2644,  0.7752,  0.0451, -0.4554,  0.8739,\n",
       "         -0.7254, -0.5298, -0.2194,  0.4091, -0.2520, -0.9970, -0.0175,  0.3414,\n",
       "         -0.2132, -0.9978, -0.9831,  0.0235, -0.0549,  0.4516, -0.9475,  0.3551,\n",
       "          0.2188,  0.9346, -0.3781, -0.4427, -0.3006, -0.7701,  0.0153, -0.4534,\n",
       "         -0.2333,  0.5573,  0.2580, -0.2838,  0.4083, -0.0581,  0.1780, -0.4115]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf9886",
   "metadata": {},
   "source": [
    "It will take a long while to compute all embeddings for the entire dataset. Here we sampled 40000 sentences from the dataset and computer SPECTER embeddings for all the sentences. Let's use the embeddings to build a simple logistic regression to predict citation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a60e4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "specter_emb = pd.read_csv('~/datasets/s4/ACL-ARC/specter_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10481c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = specter_emb.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb447b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0edbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(specter_emb, label, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e9bcd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'elasticnet', solver = 'saga', l1_ratio = 0.0, n_jobs = 4, max_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2e65c98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(l1_ratio=0.0, max_iter=1000, n_jobs=4, penalty='elasticnet',\n",
       "                   solver='saga')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ecfdbad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7328484666716651"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lliang06/anaconda3/envs/gpu/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "pred = lr.predict(X_test)\n",
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7775c77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a219b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037de719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
