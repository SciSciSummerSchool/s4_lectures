{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1798408b",
   "metadata": {},
   "source": [
    "## Text Mining  \n",
    "\n",
    "For traditional data mining, data are often presented in a \"structured\" form: thoes data are presented in tabular form.   \n",
    "As we can see from the first line of data point we just imported, for a text mining task, we are dealing with a sequence of text, which is \"unstructured\". we will need to transform the text --- an \"unstructured\" form of data, into a \"structured\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91810",
   "metadata": {},
   "source": [
    "The first step to make text data \"structured\" is to tokenize text. To tokenize text is to segment text into smaller units: a word, a character or a punctuation. After recognizing all the tokens in a dataset, we can \"tell\" the computer what to look at when processing a line of text. One way to do it is to either count how many times a token appear in a line of text, or see whether a token appears in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4956",
   "metadata": {},
   "source": [
    "Load common packages for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd0ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac7027",
   "metadata": {},
   "source": [
    "Loading the citation dataset from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a933d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8e0b42406864>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'~/datasets/s4/ACL-ARC/training.jsonl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m                 )\n\u001b[0;32m    298\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 299\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                 \u001b[0mdata_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 692\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_lines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    693\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"frame\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"series\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\pandas\\io\\json\\_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1077\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m             self.obj = DataFrame(\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m             )\n\u001b[0;32m   1081\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe558b00",
   "metadata": {},
   "source": [
    "Show the first 5 lines from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88dbc6",
   "metadata": {},
   "source": [
    "Get the first line of text. According to the label, it doesn't have citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bede893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['cur_sent'][0])\n",
    "print(df['cur_has_citation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d0bdc",
   "metadata": {},
   "source": [
    "Here, we import the functionality we need from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52a4a3",
   "metadata": {},
   "source": [
    "There are several setting we can choose for the text vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808acb",
   "metadata": {},
   "source": [
    "unigram term frequency vectorizer: each token is one word, the vectorizer count how many times a word appear in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1cc2a",
   "metadata": {},
   "source": [
    "unigram boolean vectorizer: instead of counting the word frequency, it checks whether the word appears in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581f6fc",
   "metadata": {},
   "source": [
    "unigram and bigram term frequency vectorizer: each token have up to 2 words. We are also using the built-in stop word list for English, so stopwords are not being counted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c878",
   "metadata": {},
   "source": [
    "tf-idf is a normalized version of word frequency count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a03c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38aee0",
   "metadata": {},
   "source": [
    "unigram tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75cf2e",
   "metadata": {},
   "source": [
    "fit vocabulary in texts and transform it into vectors. \"fit\" collects unique tokens into the vocabulary. \"transform\" converts each document to vector based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698879",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = unigram_count_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d06f4",
   "metadata": {},
   "source": [
    "The size of the vectorized dataset: there are 859636 data points and 261582 unigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae201db",
   "metadata": {},
   "source": [
    "As we can see here, a vecter for a line of text is sparse: most of the columns have 0 value because a vectorizer counts the appearance of all the tokens in the dataset even when a token is no in one particular line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b0802",
   "metadata": {},
   "source": [
    "The size of the vocabulary, in other words, the number of tokens in the dataset it is the size for each vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be975f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unigram_count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7889d5",
   "metadata": {},
   "source": [
    "## Classification Task with Vectorized Text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6277",
   "metadata": {},
   "source": [
    "Using the vectorized text, we can train a simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09bb47",
   "metadata": {},
   "source": [
    "In order to validate the model, we split the entire dataset into training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1355a3",
   "metadata": {},
   "source": [
    "Import logistic regression model and performance metrics from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335010da",
   "metadata": {},
   "source": [
    "Initialize the logistic regression model, setting the maximum iteration to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8c6a1",
   "metadata": {},
   "source": [
    "Fit the model with training split of the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47100a",
   "metadata": {},
   "source": [
    "Using the trained model, we make prediction with the text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1198",
   "metadata": {},
   "source": [
    "Calculate the f1 score for both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bc21",
   "metadata": {},
   "source": [
    "Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d81dee",
   "metadata": {},
   "source": [
    "Each word token correspond to a coefficient in the logistic regression. If a token is more important to the classification task, it is more likely to have a larger coefficient.In the following dataframe, we are sorting the tokens by the values of coefficients in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(unigram_count_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec635b8",
   "metadata": {},
   "source": [
    "## More Language Features with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c22d",
   "metadata": {},
   "source": [
    "There are also many more instereting feature we can get from a line of text aside from the frequency of words.  \n",
    "In the following section, we will explore more language features with the package spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Loading a pre-trained Pipeline \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the first line of sentence in our dataset with the loaded Pipeline\n",
    "tokens = nlp(df['cur_sent'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d927b",
   "metadata": {},
   "source": [
    "Print out the line of text we just passed to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3161507",
   "metadata": {},
   "source": [
    "Getting all the features generated by the Pipeline from the line of text we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_features = {}\n",
    "sentence_features['word'] = []\n",
    "sentence_features['lemma'] = []\n",
    "sentence_features['pos_tag'] = []\n",
    "sentence_features['shape'] = []\n",
    "sentence_features['is_alphabetic'] = []\n",
    "sentence_features['is_stopword'] = []\n",
    "\n",
    "for token in tokens:\n",
    "    sentence_features['word'].append(token.text)\n",
    "    sentence_features['lemma'].append(token.lemma_)\n",
    "    sentence_features['pos_tag'].append(token.pos_)\n",
    "    sentence_features['shape'].append(token.shape_)\n",
    "    sentence_features['is_alphabetic'].append(token.is_alpha)\n",
    "    sentence_features['is_stopword'].append(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a693d1",
   "metadata": {},
   "source": [
    "In the table below, we see that the Pipeline tokenized the text into words.  \n",
    "\"lemma\" is the base form of the token (word)  \n",
    "\"pos_tag\" is the pos-tagging tags for a token  \n",
    "\"shape\" shows the visual shape of the token (uppercase or lowercase, punctuation, digits)  \n",
    "\"is alphabetic\" shows whether a token is alphabetic  \n",
    "\"is stopword\" shows whether a token is a stopword  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(sentence_features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72334ffa",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539395",
   "metadata": {},
   "source": [
    "Try using tfidf vectors to train the logistic regression. In that case, what are the most important tokens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
