{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1798408b",
   "metadata": {},
   "source": [
    "## Text Mining  \n",
    "\n",
    "For traditional data mining, data are often presented in a \"structured\" form: thoes data are presented in tabular form.   \n",
    "As we can see from the first line of data point we just imported, for a text mining task, we are dealing with a sequence of text, which is \"unstructured\". we will need to transform the text --- an \"unstructured\" form of data, into a \"structured\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91810",
   "metadata": {},
   "source": [
    "The first step to make text data \"structured\" is to tokenize text. To tokenize text is to segment text into smaller units: a word, a character or a punctuation. After recognizing all the tokens in a dataset, we can \"tell\" the computer what to look at when processing a line of text. One way to do it is to either count how many times a token appear in a line of text, or see whether a token appears in the sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4956",
   "metadata": {},
   "source": [
    "Load common packages for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdd0ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac7027",
   "metadata": {},
   "source": [
    "Loading the citation dataset from the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a933d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe558b00",
   "metadata": {},
   "source": [
    "Show the first 5 lines from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49fa1e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cur_sent</th>\n",
       "      <th>cur_scaled_len_features</th>\n",
       "      <th>cur_has_citation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the system consists of two linguistically sign...</td>\n",
       "      <td>{'type': 1, 'values': [0.028884026258205003, 0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and so i has failed to conclude what a intende...</td>\n",
       "      <td>{'type': 1, 'values': [0.015098468271334, 0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>our analysis indicates human annotators achiev...</td>\n",
       "      <td>{'type': 1, 'values': [0.031509846827133, 0.03...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mcveigh who s no relation of the convicted okl...</td>\n",
       "      <td>{'type': 1, 'values': [0.016849015317286, 0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>each synset of italwordnet is linked to the in...</td>\n",
       "      <td>{'type': 1, 'values': [0.014004376367614, 0.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cur_sent  \\\n",
       "0  the system consists of two linguistically sign...   \n",
       "1  and so i has failed to conclude what a intende...   \n",
       "2  our analysis indicates human annotators achiev...   \n",
       "3  mcveigh who s no relation of the convicted okl...   \n",
       "4  each synset of italwordnet is linked to the in...   \n",
       "\n",
       "                             cur_scaled_len_features  cur_has_citation  \n",
       "0  {'type': 1, 'values': [0.028884026258205003, 0...                 0  \n",
       "1  {'type': 1, 'values': [0.015098468271334, 0.01...                 0  \n",
       "2  {'type': 1, 'values': [0.031509846827133, 0.03...                 0  \n",
       "3  {'type': 1, 'values': [0.016849015317286, 0.01...                 0  \n",
       "4  {'type': 1, 'values': [0.014004376367614, 0.01...                 0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88dbc6",
   "metadata": {},
   "source": [
    "Get the first line of text. According to the label, it doesn't have citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bede893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the system consists of two linguistically significant parts a machine lexicon residing on a direct access device and a program package\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df['cur_sent'][0])\n",
    "print(df['cur_has_citation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d0bdc",
   "metadata": {},
   "source": [
    "Here, we import the functionality we need from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c0f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52a4a3",
   "metadata": {},
   "source": [
    "There are several setting we can choose for the text vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808acb",
   "metadata": {},
   "source": [
    "unigram term frequency vectorizer: each token is one word, the vectorizer count how many times a word appear in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc7bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1cc2a",
   "metadata": {},
   "source": [
    "unigram boolean vectorizer: instead of counting the word frequency, it checks whether the word appears in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7587cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581f6fc",
   "metadata": {},
   "source": [
    "unigram and bigram term frequency vectorizer: each token have up to 2 words. We are also using the built-in stop word list for English, so stopwords are not being counted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9388dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c878",
   "metadata": {},
   "source": [
    "tf-idf is a normalized version of word frequency count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a03c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38aee0",
   "metadata": {},
   "source": [
    "unigram tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75cf2e",
   "metadata": {},
   "source": [
    "fit vocabulary in texts and transform it into vectors. \"fit\" collects unique tokens into the vocabulary. \"transform\" converts each document to vector based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1698879",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = unigram_count_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d06f4",
   "metadata": {},
   "source": [
    "The size of the vectorized dataset: there are 859636 data points and 261582 unigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa2d54cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(859636, 261582)\n"
     ]
    }
   ],
   "source": [
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae201db",
   "metadata": {},
   "source": [
    "As we can see here, a vecter for a line of text is sparse: most of the columns have 0 value because a vectorizer counts the appearance of all the tokens in the dataset even when a token is no in one particular line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a2b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(word_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b0802",
   "metadata": {},
   "source": [
    "The size of the vocabulary, in other words, the number of tokens in the dataset it is the size for each vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be975f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261582\n"
     ]
    }
   ],
   "source": [
    "print(len(unigram_count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7889d5",
   "metadata": {},
   "source": [
    "## Classification Task with Vectorized Text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6277",
   "metadata": {},
   "source": [
    "Using the vectorized text, we can train a simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09bb47",
   "metadata": {},
   "source": [
    "In order to validate the model, we split the entire dataset into training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1355a3",
   "metadata": {},
   "source": [
    "Import logistic regression model and performance metrics from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335010da",
   "metadata": {},
   "source": [
    "Initialize the logistic regression model, setting the maximum iteration to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8c6a1",
   "metadata": {},
   "source": [
    "Fit the model with training split of the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47100a",
   "metadata": {},
   "source": [
    "Using the trained model, we make prediction with the text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1198",
   "metadata": {},
   "source": [
    "Calculate the f1 score for both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bc21",
   "metadata": {},
   "source": [
    "Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d81dee",
   "metadata": {},
   "source": [
    "Each word token correspond to a coefficient in the logistic regression. If a token is more important to the classification task, it is more likely to have a larger coefficient.In the following dataframe, we are sorting the tokens by the values of coefficients in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(unigram_count_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec635b8",
   "metadata": {},
   "source": [
    "## More Language Features with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c22d",
   "metadata": {},
   "source": [
    "There are also many more instereting feature we can get from a line of text aside from the frequency of words.  \n",
    "In the following section, we will explore more language features with the package spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Loading a pre-trained Pipeline \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the first line of sentence in our dataset with the loaded Pipeline\n",
    "tokens = nlp(df['cur_sent'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d927b",
   "metadata": {},
   "source": [
    "Print out the line of text we just passed to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3161507",
   "metadata": {},
   "source": [
    "Getting all the features generated by the Pipeline from the line of text we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_features = {}\n",
    "sentence_features['word'] = []\n",
    "sentence_features['lemma'] = []\n",
    "sentence_features['pos_tag'] = []\n",
    "sentence_features['shape'] = []\n",
    "sentence_features['is_alphabetic'] = []\n",
    "sentence_features['is_stopword'] = []\n",
    "\n",
    "for token in tokens:\n",
    "    sentence_features['word'].append(token.text)\n",
    "    sentence_features['lemma'].append(token.lemma_)\n",
    "    sentence_features['pos_tag'].append(token.pos_)\n",
    "    sentence_features['shape'].append(token.shape_)\n",
    "    sentence_features['is_alphabetic'].append(token.is_alpha)\n",
    "    sentence_features['is_stopword'].append(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a693d1",
   "metadata": {},
   "source": [
    "In the table below, we see that the Pipeline tokenized the text into words.  \n",
    "\"lemma\" is the base form of the token (word)  \n",
    "\"pos_tag\" is the pos-tagging tags for a token  \n",
    "\"shape\" shows the visual shape of the token (uppercase or lowercase, punctuation, digits)  \n",
    "\"is alphabetic\" shows whether a token is alphabetic  \n",
    "\"is stopword\" shows whether a token is a stopword  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(sentence_features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72334ffa",
   "metadata": {},
   "source": [
    "## Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539395",
   "metadata": {},
   "source": [
    "Try using tfidf vectors to train the logistic regression. In that case, what are the most important tokens?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
