{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1798408b",
   "metadata": {},
   "source": [
    "# Natural Language Processing & Text Mining  \n",
    "\n",
    "For normal data mining and machine learning tasks, data are often presented in a \"structured\" form: thoes data are presented in tabular form.   \n",
    "As we can see from the first line of data point we just imported, for a text mining task, we are dealing with a sequence of text, which is \"unstructured\". we will need to transform the text --- an \"unstructured\" form of data, into a \"structured\" form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e91810",
   "metadata": {},
   "source": [
    "The first step to make text data \"structured\" is to tokenize text. To tokenize text is to segment text into smaller units: a word, a character or a punctuation. After recognizing all the tokens in a dataset, we can \"tell\" the computer what to look at when processing a line of text. One way to do it is to either count how many times a token appear in a line of text, or see whether a token appears in the sentence (the bag-of-word-model). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38c4956",
   "metadata": {},
   "source": [
    "Load common packages for data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd0ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac7027",
   "metadata": {},
   "source": [
    "Loading the ACLâ€‘ARC dataset from the data folder        \n",
    "The dataset could be downloaded from https://figshare.com/articles/dataset/ACL-ARC_dataset/12573872    \n",
    "For this demo, we will only use the train split and do cross-validation with the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a933d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('~/datasets/s4/ACL-ARC/training.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe558b00",
   "metadata": {},
   "source": [
    "Show the first 5 lines from the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fa1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb88dbc6",
   "metadata": {},
   "source": [
    "Get the first line of text. According to the label, it doesn't have citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bede893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['cur_sent'][0])\n",
    "print(df['cur_has_citation'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d0bdc",
   "metadata": {},
   "source": [
    "Here, we import the functionality we need from scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c0f4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52a4a3",
   "metadata": {},
   "source": [
    "There are several setting we can choose for the text vectorizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99808acb",
   "metadata": {},
   "source": [
    "unigram term frequency vectorizer: each token is one word, the vectorizer count how many times a word appear in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7bf049",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer = CountVectorizer(encoding='latin-1', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1cc2a",
   "metadata": {},
   "source": [
    "unigram boolean vectorizer: instead of counting the word frequency, it checks whether the word appears in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7587cc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_bool_vectorizer = CountVectorizer(encoding='latin-1', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581f6fc",
   "metadata": {},
   "source": [
    "unigram and bigram term frequency vectorizer: each token have up to 2 words. We are also using the built-in stop word list for English, so stopwords are not being counted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9388dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_count_vectorizer = CountVectorizer(encoding='latin-1', ngram_range=(1,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef62c878",
   "metadata": {},
   "source": [
    "tf-idf is a normalized version of word frequency count     \n",
    "unigram tfidf vectorizer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e428af",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_tfidf_vectorizer = TfidfVectorizer(encoding='latin-1', use_idf=True, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75cf2e",
   "metadata": {},
   "source": [
    "fit vocabulary in texts and transform it into vectors. \"fit\" collects unique tokens into the vocabulary. \"transform\" converts each document to vector based on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1698879",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = unigram_count_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d06f4",
   "metadata": {},
   "source": [
    "The size of the vectorized dataset: there are 859636 data points and 261582 unigram tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d54cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae201db",
   "metadata": {},
   "source": [
    "As we can see here, a vecter for a line of text is sparse: most of the columns have 0 value because a vectorizer counts the appearance of all the tokens in the dataset even when a token is no in one particular line of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vector[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b0802",
   "metadata": {},
   "source": [
    "The size of the vocabulary, in other words, the number of tokens in the dataset it is the size for each vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be975f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unigram_count_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7889d5",
   "metadata": {},
   "source": [
    "## Classification Task with Vectorized Text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6277",
   "metadata": {},
   "source": [
    "Using the vectorized text, we can train a simple logistic regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09bb47",
   "metadata": {},
   "source": [
    "In order to validate the model, we split the entire dataset into training dataset and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7223444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1355a3",
   "metadata": {},
   "source": [
    "Import logistic regression model and performance metrics from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d35b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335010da",
   "metadata": {},
   "source": [
    "Initialize the logistic regression model, setting the maximum iteration to 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c8c6a1",
   "metadata": {},
   "source": [
    "Fit the model with training split of the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a7580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47100a",
   "metadata": {},
   "source": [
    "Using the trained model, we make prediction with the text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cb1198",
   "metadata": {},
   "source": [
    "Calculate the f1 score for both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb94bc21",
   "metadata": {},
   "source": [
    "Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d81dee",
   "metadata": {},
   "source": [
    "Each word token correspond to a coefficient in the logistic regression. If a token is more important to the classification task, it is more likely to have a larger coefficient.In the following dataframe, we are sorting the tokens by the values of coefficients in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(unigram_count_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e817f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98e6e3cf",
   "metadata": {},
   "source": [
    "Next, we will try out ti-idf: a normalized form of bag-of-word representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word_vector = unigram_tfidf_vectorizer.fit_transform(df['cur_sent'].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5c3ca",
   "metadata": {},
   "source": [
    "Split the dataset into training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2455a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_word_vector, df['cur_has_citation'], test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a74a5",
   "metadata": {},
   "source": [
    "Initialize the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ece5e9",
   "metadata": {},
   "source": [
    "Fit the model with training split of the vectorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6447c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e650cd20",
   "metadata": {},
   "source": [
    "Using the trained model, we make prediction with the text split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07893638",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e10c8a3",
   "metadata": {},
   "source": [
    "Calculate the f1 score for both positive and negative class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf7acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4a553",
   "metadata": {},
   "source": [
    "Calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b2195",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc396f",
   "metadata": {},
   "source": [
    "Each word token correspond to a coefficient in the logistic regression. If a token is more important to the classification task, it is more likely to have a larger coefficient.In the following dataframe, we are sorting the tokens by the values of coefficients in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18999dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame(unigram_tfidf_vectorizer.get_feature_names(), columns=['word']), \n",
    "           pd.DataFrame(clf.coef_.transpose(), columns=['coef'])], axis = 1).sort_values(by = 'coef', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ee8d9",
   "metadata": {},
   "source": [
    "With different vectorization methods, we will get different performance for our model and different model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec13551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ec635b8",
   "metadata": {},
   "source": [
    "## More Language Features with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05c22d",
   "metadata": {},
   "source": [
    "There are also many more instereting feature we can get from a line of text aside from the frequency of words.  \n",
    "In the following section, we will explore more language features with the package spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7c389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#installing spacy\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Loading a pre-trained Pipeline \n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process the first line of sentence in our dataset with the loaded Pipeline\n",
    "tokens = nlp(df['cur_sent'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d927b",
   "metadata": {},
   "source": [
    "Print out the line of text we just passed to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f0e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3161507",
   "metadata": {},
   "source": [
    "Getting all the features generated by the Pipeline from the line of text we passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469c2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_features = {}\n",
    "sentence_features['word'] = []\n",
    "sentence_features['lemma'] = []\n",
    "sentence_features['pos_tag'] = []\n",
    "sentence_features['shape'] = []\n",
    "sentence_features['is_alphabetic'] = []\n",
    "sentence_features['is_stopword'] = []\n",
    "\n",
    "for token in tokens:\n",
    "    sentence_features['word'].append(token.text)\n",
    "    sentence_features['lemma'].append(token.lemma_)\n",
    "    sentence_features['pos_tag'].append(token.pos_)\n",
    "    sentence_features['shape'].append(token.shape_)\n",
    "    sentence_features['is_alphabetic'].append(token.is_alpha)\n",
    "    sentence_features['is_stopword'].append(token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a693d1",
   "metadata": {},
   "source": [
    "In the table below, we see that the Pipeline tokenized the text into words.  \n",
    "\"lemma\" is the base form of the token (word)  \n",
    "\"pos_tag\" is the pos-tagging tags for a token  \n",
    "\"shape\" shows the visual shape of the token (uppercase or lowercase, punctuation, digits)  \n",
    "\"is alphabetic\" shows whether a token is alphabetic  \n",
    "\"is stopword\" shows whether a token is a stopword  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c89db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.DataFrame(sentence_features).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b26c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053eaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b31c71",
   "metadata": {},
   "source": [
    "Getting the abstract of the famous \"Science of Science\" review paper    \n",
    "(Fortunato, S., Bergstrom, C. T., BÃ¶rner, K., Evans, J. A., Helbing, D., MilojeviÄ‡, S., ... & BarabÃ¡si, A. L. (2018). Science of science. Science, 359(6379), eaao0185.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_paper = requests.get(\n",
    "    'https://api.openalex.org/works/https://doi.org/10.1126/science.aao0185'\n",
    ").json()\n",
    "\n",
    "abstract_inverted_index = SOS_paper['abstract_inverted_index']\n",
    "\n",
    "max_ids = 0\n",
    "for k in abstract_inverted_index.keys():\n",
    "    for i in abstract_inverted_index[k]:\n",
    "        if i > max_ids:\n",
    "            max_ids = i\n",
    "\n",
    "abstract = [' '] * (max_ids + 1)\n",
    "\n",
    "for k in abstract_inverted_index.keys():\n",
    "    for i in abstract_inverted_index[k]:\n",
    "        abstract[i] = k\n",
    "\n",
    "abstract.remove('BACKGROUND')\n",
    "abstract.remove('ADVANCES')\n",
    "abstract.remove('OUTLOOK')\n",
    "\n",
    "while(\" \" in abstract) :\n",
    "    abstract.remove(\" \")\n",
    "     \n",
    "abstract = ' '.join(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9fdc00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac53aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1de3d3",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f888f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c194b6d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lemmatization is the process of reducing inflected forms, sometimes derivationally related forms of a word to a common base form. This reduced form or root word is called a lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16a1ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = \"am are is\"\n",
    "[token.lemma_ for token in nlp(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae24823",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text = \"look looks looked\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35aeba5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44db44f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(\"token:{} -> lemma:{}\".format(token.text,token.lemma_ ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1ed15",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dd06d2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bdbd8",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(abstract)\n",
    "words =  [token.text for token in doc if token.is_punct != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57455e93",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "word_counter = Counter(words)\n",
    "word_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d145d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10694194",
   "metadata": {},
   "source": [
    "Stopwords usually don't contribute a lot to the semantic meaning of sentence. In many cases we remove those stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e32f7b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c956a56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9e5cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "list(spacy_stopwords)[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee44e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "no_stop_words = [token for token in doc if not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129052ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113f5e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Token Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db5610",
   "metadata": {},
   "source": [
    "For each token in a spacy document, there are related attributes, such as, what is the lemma of the token, is the token a stopword? is the token alphabetical? etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6254f864",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cols = (\"text\", \"lemma_\",\"is_punct\", \"is_stop\", \"is_alpha\", \"is_space\", \"lower_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e4521",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rows = [] \n",
    "for t in doc:\n",
    "    row = [t.text, t.lemma_,  t.is_punct,  t.is_stop,  t.is_alpha,  t.is_space,  t.lower_]\n",
    "    rows.append(row)\n",
    "attri_pdf = pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8336a6d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "attri_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6d135",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50224d75",
   "metadata": {},
   "source": [
    "Spacy breaks a document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d8a3a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(\"start_pos={}, end_pos={}, text:{}\".format(sent.start, sent.end, sent.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be82ce5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ce9c2",
   "metadata": {},
   "source": [
    "Associate each word in a text with its correct lexical-syntactic category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadc511b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2e0fa3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a7311",
   "metadata": {},
   "source": [
    "Dependency grammars represent syntactic dependency relations between words that show the syntactic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e72f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for token in next(doc.sents):\n",
    "    print(\"{0}/{1} <--{2}-- {3}/{4}\".format(\n",
    "        token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a856f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(next(doc.sents), style='dep', jupyter=True, options={'distance': 90})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd1cb3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad91d05",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052e14a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be1703",
   "metadata": {},
   "source": [
    "Word embeddings, such as word2vec, are deep learning methods to generate word representation that includes semantic meaning of words in a fixed-sized numerical vector. Semantic information of each word are captured by the context of those word in the corpus (training data for word embedding models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3a549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "man = nlp.vocab[\"man\"]\n",
    "woman = nlp.vocab[\"woman\"]\n",
    "king = nlp.vocab[\"king\"]\n",
    "queen = nlp.vocab[\"queen\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136788b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "queen.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe503a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "queen.similarity(king)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4680bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "queen.similarity(woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d690c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "man.similarity(woman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a56c06",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine(x,y):\n",
    "    return np.dot(x,y) / (np.sqrt(np.dot(x,x)) * np.sqrt(np.dot(y,y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b388dfc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cosine(king.vector-man.vector+woman.vector, queen.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80af84a4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By default, Token.vector returns the vector for its underlying Lexeme, while Doc.vector and Span.vector return an average of the vectors of their tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31840c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1 = nlp(\"The quick brown fox jumps over the lazy dog\")\n",
    "doc2 = nlp(\"The lazy dog jumps over the quick brown fox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4664dbbd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1.similarity(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f913876d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc1_vec=doc1.vector\n",
    "doc2_vec=doc2.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7e698",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cosine(doc2_vec,doc1_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a6629",
   "metadata": {},
   "source": [
    "### Try it yourself!    \n",
    "Using word embeddings for the citation worthiness classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8d0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdec556d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "References: \n",
    "- https://scikit-learn.org/stable/\n",
    "- https://spacy.io/\n",
    "- https://github.com/tong-zeng/spaCy_tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
